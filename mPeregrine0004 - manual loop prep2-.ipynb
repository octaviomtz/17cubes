{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep categorical cross entropy, try RMSprop for optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cubesNumber=17\n",
    "foldX=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readScan(scan):\n",
    "    # We read the file saved in Matlab. There is only one variable in the file called scansMini\n",
    "    data = h5py.File(scan, 'r')\n",
    "    Xscans=data.get('scansMini')\n",
    "    # We have to get the values into the right format (subjects, dim1, dim2, dim3, channels)\n",
    "    X=copy.copy(Xscans.value)\n",
    "    X=np.expand_dims(X,4)\n",
    "    X1=np.rollaxis(X,3)\n",
    "    return X1\n",
    "    \n",
    "############\n",
    "\n",
    "def readLabels(labels):\n",
    "    data = h5py.File(labels, 'r')\n",
    "    Xscans=data.get('labels')\n",
    "    X=copy.copy(Xscans.value)\n",
    "    X2=np.squeeze(X).astype(int)\n",
    "    X3= np.zeros((len(X2), 2))\n",
    "    X3[np.arange(len(X2)), X2] = 1\n",
    "    return X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gSize=6\n",
    "dropoutGrid = np.random.uniform(0,.4, gSize)     #param1\n",
    "LRGrid = 10 ** np.random.uniform(-6, 0, gSize)   #param2\n",
    "L2penal1Grid = 10 ** np.random.uniform(-5, 0, gSize)  #param3\n",
    "L2penal2Grid = 10 ** np.random.uniform(-5, 0, gSize)   #param4\n",
    "optimizerGrid=['adam','RMSprop']   #param5\n",
    "activationsGrid=['relu','elu']   #param6\n",
    "neurons1=3**np.random.uniform(1,4,gSize)   #param7\n",
    "neurons1Grid=[np.floor(i) for i in neurons1]\n",
    "neurons2=3**np.random.uniform(1,4,gSize)   #param8\n",
    "neurons2Grid=[np.floor(i) for i in neurons2]\n",
    "neurons3=3**np.random.uniform(1,4,gSize)   #param9\n",
    "neurons3Grid=[np.floor(i) for i in neurons3]\n",
    "miniBatches=np.random.uniform(4,50,gSize)\n",
    "miniBatchGrid=[np.floor(i) for i in miniBatches] #param12\n",
    "\n",
    "kernelsLayer1=3**np.random.uniform(1,4,gSize)   \n",
    "kernelsLayer1Grid=[np.floor(i) for i in kernelsLayer1] #param10\n",
    "\n",
    "ratioBetweenKernels=np.random.uniform(1,2)   \n",
    "kernelslay2=kernelsLayer1*ratioBetweenKernels\n",
    "kernelsLayer2Grid=([np.floor(i) for i in kernelslay2])#param11\n",
    "\n",
    "print (gSize**6)*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mixN=(gSize**6)*2*2\n",
    "print(np.arange(len(dropoutGrid)))\n",
    "print(np.arange(len(LRGrid)))\n",
    "print(np.arange(len(L2penal1Grid)))\n",
    "print(np.arange(len(optimizerGrid)))\n",
    "print(np.arange(len(activationsGrid)))\n",
    "print(np.arange(len(neurons1Grid)))\n",
    "print(np.arange(len(miniBatchGrid)))\n",
    "print(np.arange(len(kernelsLayer1Grid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ratioBetweenKernels\n",
    "for idx, i in enumerate(kernelsLayer1):\n",
    "    print kernelsLayer1Grid[idx], kernelsLayer2Grid[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelName='p0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Merge\n",
    "from keras.layers.convolutional import Convolution3D\n",
    "from keras.layers.pooling import MaxPooling3D\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "\n",
    "def readScan(scan):\n",
    "    # We read the file saved in Matlab. There is only one variable in the file called scansMini\n",
    "    data = h5py.File(scan, 'r')\n",
    "    Xscans=data.get('scansMini')\n",
    "    # We have to get the values into the right format (subjects, dim1, dim2, dim3, channels)\n",
    "    X=copy.copy(Xscans.value)\n",
    "    X=np.expand_dims(X,4)\n",
    "    X1=np.rollaxis(X,3)\n",
    "    return X1\n",
    "    \n",
    "############\n",
    "\n",
    "def readLabels(labels):\n",
    "    data = h5py.File(labels, 'r')\n",
    "    Xscans=data.get('labels')\n",
    "    X=copy.copy(Xscans.value)\n",
    "    X2=np.squeeze(X).astype(int)\n",
    "    X3= np.zeros((len(X2), 2))\n",
    "    X3[np.arange(len(X2)), X2] = 1\n",
    "    return X3\n",
    "    \n",
    "############\n",
    "\n",
    "def getScansFold(foldX,cubesNumber):\n",
    "    foldName='fold{0}'.format(foldX)\n",
    "    XTrain=[]\n",
    "    XTest=[]\n",
    "    for i in (np.arange(cubesNumber)+1):\n",
    "        pathNameTrain=foldName+'/train/scansMiniTrain{0}.mat'.format(i)\n",
    "        pathNameTest=foldName+'/test/scansMiniTest{0}.mat'.format(i)\n",
    "        xtrain=readScan(pathNameTrain)\n",
    "        xtest=readScan(pathNameTest)\n",
    "        XTrain.append(xtrain)\n",
    "        XTest.append(xtest)\n",
    "    pathNameLabelTrain=foldName+'/train/labelsTrain.mat'\n",
    "    pathNameLabelTest=foldName+'/test/labelsTest.mat'\n",
    "    y=readLabels(pathNameLabelTrain)\n",
    "    y_true=readLabels(pathNameLabelTest)\n",
    "    return XTrain, XTest, y, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "\n",
    "def restAllModels():\n",
    "    final_model.reset_states()\n",
    "    model1.reset_states()\n",
    "    model2.reset_states()\n",
    "    model3.reset_states()\n",
    "    model4.reset_states()\n",
    "    model5.reset_states()\n",
    "    model6.reset_states()\n",
    "    model7.reset_states()\n",
    "    model8.reset_states()\n",
    "    model9.reset_states()\n",
    "    model10.reset_states()\n",
    "    model11.reset_states()\n",
    "    model12.reset_states()\n",
    "    model13.reset_states()\n",
    "    model14.reset_states()\n",
    "    model15.reset_states()\n",
    "    model16.reset_states()\n",
    "    model17.reset_states()\n",
    "    \n",
    "###########\n",
    "\n",
    "adam=Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-04)\n",
    "conv1aW=0.05\n",
    "conv1bW=0.05\n",
    "conv1aN=20\n",
    "conv1aInit='he_normal'\n",
    "conv1aBorder='valid'\n",
    "conv1aAct='relu'\n",
    "fully1W=0.0\n",
    "fully1N=20\n",
    "fully1act='tanh'\n",
    "fully1init='glorot_normal'\n",
    "fully1drop=0.2\n",
    "fully2N=10\n",
    "fully2W=.01\n",
    "fully2init='glorot_normal'\n",
    "fully2act='tanh'\n",
    "outInit='glorot_normal'\n",
    "#outAct='tanh'\n",
    "outAct='softmax'\n",
    "outW=.05\n",
    "#Loss='binary_crossentropy'\n",
    "Loss='categorical_crossentropy'\n",
    "\n",
    "###########\n",
    "\n",
    "model1=Sequential()\n",
    "model1.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation(conv1aAct))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dropout(fully1drop))\n",
    "model1.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model2=Sequential()\n",
    "model2.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder,  init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(conv1aAct))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(fully1drop))\n",
    "model2.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model3=Sequential()\n",
    "model3.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder,  init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation(conv1aAct))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(fully1drop))\n",
    "model3.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model4=Sequential()\n",
    "model4.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder,  init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation(conv1aAct))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dropout(fully1drop))\n",
    "model4.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model5=Sequential()\n",
    "model5.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation(conv1aAct))\n",
    "model5.add(Flatten())\n",
    "model5.add(Dropout(fully1drop))\n",
    "model5.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model6=Sequential()\n",
    "model6.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation(conv1aAct))\n",
    "model6.add(Flatten())\n",
    "model6.add(Dropout(fully1drop))\n",
    "model6.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model7=Sequential()\n",
    "model7.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder,  init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(Activation(conv1aAct))\n",
    "model7.add(Flatten())\n",
    "model7.add(Dropout(fully1drop))\n",
    "model7.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model8=Sequential()\n",
    "model8.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(Activation(conv1aAct))\n",
    "model8.add(Flatten())\n",
    "model8.add(Dropout(fully1drop))\n",
    "model8.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model9=Sequential()\n",
    "model9.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model9.add(BatchNormalization())\n",
    "model9.add(Activation(conv1aAct))\n",
    "model9.add(Flatten())\n",
    "model9.add(Dropout(fully1drop))\n",
    "model9.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model10=Sequential()\n",
    "model10.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model10.add(BatchNormalization())\n",
    "model10.add(Activation(conv1aAct))\n",
    "model10.add(Flatten())\n",
    "model10.add(Dropout(fully1drop))\n",
    "model10.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model11=Sequential()\n",
    "model11.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model11.add(BatchNormalization())\n",
    "model11.add(Activation(conv1aAct))\n",
    "model11.add(Flatten())\n",
    "model11.add(Dropout(fully1drop))\n",
    "model11.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model12=Sequential()\n",
    "model12.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model12.add(BatchNormalization())\n",
    "model12.add(Activation(conv1aAct))\n",
    "model12.add(Flatten())\n",
    "model12.add(Dropout(fully1drop))\n",
    "model12.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model13=Sequential()\n",
    "model13.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model13.add(BatchNormalization())\n",
    "model13.add(Activation(conv1aAct))\n",
    "model13.add(Flatten())\n",
    "model13.add(Dropout(fully1drop))\n",
    "model13.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model14=Sequential()\n",
    "model14.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model14.add(BatchNormalization())\n",
    "model14.add(Activation(conv1aAct))\n",
    "model14.add(Flatten())\n",
    "model14.add(Dropout(fully1drop))\n",
    "model14.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model15=Sequential()\n",
    "model15.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model15.add(BatchNormalization())\n",
    "model15.add(Activation(conv1aAct))\n",
    "model15.add(Flatten())\n",
    "model15.add(Dropout(fully1drop))\n",
    "model15.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model16=Sequential()\n",
    "model16.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model16.add(BatchNormalization())\n",
    "model16.add(Activation(conv1aAct))\n",
    "model16.add(Flatten())\n",
    "model16.add(Dropout(fully1drop))\n",
    "model16.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "model17=Sequential()\n",
    "model17.add(Convolution3D(conv1aN, 3, 3, 3, input_shape=(10, 10, 10, 1), border_mode=conv1aBorder, init=conv1aInit, W_regularizer=l2(conv1aW)))\n",
    "model17.add(BatchNormalization())\n",
    "model17.add(Activation(conv1aAct))\n",
    "model17.add(Flatten())\n",
    "model17.add(Dropout(fully1drop))\n",
    "model17.add(Dense(fully1N, init=fully1init, activation=fully1act, W_regularizer=l2(fully1W))) \n",
    "\n",
    "merged = Merge([model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12, model13, model14, model15, model16, model17], mode='concat')\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(fully2N, init=fully2init, activation=fully2act,W_regularizer=l2(fully2W)))\n",
    "final_model.add(Dense(2, init=outInit, activation=outAct,W_regularizer=l2(outW)))\n",
    "final_model.compile(loss=Loss, optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 198 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "265s - loss: 5.7173 - acc: 0.6818 - val_loss: 0.4000 - val_acc: 0.8800\n",
      "Train on 198 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "249s - loss: 4.9300 - acc: 0.7424 - val_loss: 0.3324 - val_acc: 0.8600\n",
      "0.160344234175\n",
      "62/62 [==============================] - 5s     \n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "\n",
    "XTrain1, XTest1, y1, y_true1 = getScansFold(1,17)\n",
    "\n",
    "acc_train=[]\n",
    "loss_train=[]\n",
    "acc_val=[]\n",
    "loss_val=[]\n",
    "all_lr=[]\n",
    "t0 = time.time()\n",
    "\n",
    "# thresholds\n",
    "thres1=.80\n",
    "thres1passed=0\n",
    "thres2=.90\n",
    "thres2passed=0\n",
    "thres3=.93\n",
    "thres3passed=0\n",
    "thres4=.96\n",
    "thres4passed=0\n",
    "maxAcc=0\n",
    "maxAccVal=0\n",
    "countNoIncrease=0\n",
    "minValLoss=10\n",
    "\n",
    "#for iteration in np.linspace(1,70,70):\n",
    "iteration=1\n",
    "while iteration<=2:\n",
    "    \n",
    "    history=final_model.fit(XTrain1, y1, validation_split=0.2, nb_epoch=1, batch_size=8,verbose=2)\n",
    "    # Append values\n",
    "    acc_train.append(history.history['acc'])\n",
    "    acc_val.append(history.history['val_acc'])\n",
    "    loss_train.append(history.history['loss'])\n",
    "    loss_val.append(history.history['val_loss'])\n",
    "    all_lr.append(K.get_value(final_model.optimizer.lr))\n",
    "    \n",
    "    ## if we have a bad initialization\n",
    "    if acc_val[-1][0] < 0.00001:\n",
    "        iteration=1\n",
    "        print('reset weights')\n",
    "        random.seed(np.random.randint(100))\n",
    "        restAllModels()\n",
    "        acc_train=[]\n",
    "        loss_train=[]\n",
    "        acc_val=[]\n",
    "        loss_val=[]\n",
    "        all_lr=[]\n",
    "        continue\n",
    "    \n",
    "    # Compare last iteration vs max value of Acc\n",
    "    if acc_train[-1][0]>maxAcc:\n",
    "        maxAcc=max(acc_train)[0]\n",
    "        maxIt=iteration\n",
    "    else:\n",
    "        countNoIncrease=countNoIncrease+1\n",
    "        \n",
    "    if acc_val[-1][0]>maxAccVal:\n",
    "        maxAccVal=max(acc_val)[0]\n",
    "               \n",
    "        if acc_val>.9:\n",
    "            #save all\n",
    "            final_model.save('k{0}val.h5'.format(modelName))\n",
    "            K_LR=K.get_value(adam.lr)\n",
    "            nameVal='k{0}val.pickle'.format(modelName)\n",
    "            iterationVal=iteration\n",
    "            with open(nameVal, 'wb') as f:\n",
    "                pickle.dump([K_LR, acc_train, loss_train, acc_val, loss_val, all_lr, countNoIncrease, maxAcc, maxAccVal, \n",
    "                             thres1, thres1passed, thres2, thres2passed, thres3, thres3passed, thres4, thres4passed,\n",
    "                            iteration], f)\n",
    "\n",
    "        \n",
    "    if loss_val[-1][0]<minValLoss:\n",
    "        minValLoss=min(loss_val)[0]\n",
    "        \n",
    "        if acc_val>.9:\n",
    "            final_model.save('k{0}loss.h5'.format(modelName))\n",
    "            #save all\n",
    "            K_LR=K.get_value(adam.lr)\n",
    "            iterationLoss=iteration\n",
    "            #nameAndIter='p{0}_loss_{1}.pickle'.format(modelName,iteration)\n",
    "            nameLoss='k{0}loss.pickle'.format(modelName)\n",
    "            with open(nameLoss, 'wb') as f:\n",
    "                pickle.dump([K_LR, acc_train, loss_train, acc_val, loss_val, all_lr, countNoIncrease, maxAcc, maxAccVal, \n",
    "                             thres1, thres1passed, thres2, thres2passed, thres3, thres3passed, thres4, thres4passed,\n",
    "                            iteration], f)\n",
    "\n",
    "    \n",
    "    if countNoIncrease>=10:\n",
    "        countNoIncrease=0\n",
    "        K.set_value(final_model.optimizer.lr, 0.5 * K.get_value(final_model.optimizer.lr))    \n",
    "        \n",
    "    if acc_train[-1][0] > thres1 and thres1passed == 0:\n",
    "        thres1passed=1\n",
    "        countNoIncrease=0\n",
    "        K.set_value(final_model.optimizer.lr, 0.5 * K.get_value(final_model.optimizer.lr))    \n",
    "        \n",
    "    if acc_train[-1][0] > thres2 and thres2passed == 0:\n",
    "        thres2passed=1\n",
    "        countNoIncrease=0\n",
    "        K.set_value(final_model.optimizer.lr, 0.5 * K.get_value(final_model.optimizer.lr))    \n",
    "        \n",
    "    if acc_train[-1][0] > thres3 and thres3passed == 0:\n",
    "        thres3passed=1\n",
    "        countNoIncrease=0\n",
    "        K.set_value(final_model.optimizer.lr, 0.5 * K.get_value(final_model.optimizer.lr))    \n",
    "        \n",
    "    if acc_train[-1][0] > thres4 and thres4passed == 0:\n",
    "        thres4passed=1\n",
    "        countNoIncrease=0\n",
    "        K.set_value(final_model.optimizer.lr, 0.5 * K.get_value(final_model.optimizer.lr))   \n",
    "        \n",
    "    iteration+=1\n",
    "    \n",
    "t1 = time.time()\n",
    "print (t1-t0)/60/60\n",
    "\n",
    "###########\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import  accuracy_score\n",
    "best_model=load_model('k{0}val.h5'.format(modelName))\n",
    "y_pred=best_model.predict_classes(XTest1)\n",
    "y_predC=np.squeeze(y_pred)  \n",
    "y_predProb=final_model.predict(XTest1)\n",
    "y_predP=np.squeeze(y_predProb)\n",
    "\n",
    "testAcc=accuracy_score(y_true1[:,1],y_predC)\n",
    "testWrong=np.where(y_true1[:,1].astype(bool)!=y_predC.astype(bool))[0]\n",
    "testMatrix=confusion_matrix(y_true1[:,1],y_predC)\n",
    "\n",
    "##########\n",
    "\n",
    "df1=pd.DataFrame(acc_train,columns=['acc_train'])\n",
    "df2=pd.DataFrame(acc_val,columns=['acc_val'])\n",
    "df3=pd.DataFrame(loss_train,columns=['loss_train'])\n",
    "df4=pd.DataFrame(loss_val,columns=['loss_val'])\n",
    "df5=pd.DataFrame([iteration],columns=['iteration'])\n",
    "df6=pd.DataFrame([iterationVal],columns=['iterationVal'])\n",
    "df7=pd.DataFrame([iterationLoss],columns=['iterationLoss'])\n",
    "df8=pd.DataFrame(all_lr,columns=['learningRate'])\n",
    "df9=pd.DataFrame([testAcc],columns=['testAcc'])\n",
    "\n",
    "outResults=pd.concat([df1,df2, df3, df4, df5, df6, df7, df8, df9],axis=1)\n",
    "wrongPredictions=pd.DataFrame(testWrong,columns=['wrongPredictions'])\n",
    "confusionMatrix=pd.DataFrame(testMatrix,columns=['0','1'])\n",
    "predictionsC=pd.DataFrame(y_predC,columns=['predClasses'])\n",
    "predictionsP=pd.DataFrame(y_predP[:,1],columns=['predProba'])\n",
    "predictions=pd.concat([predictionsC,predictionsP],axis=1)\n",
    "\n",
    "outResults.to_csv('outResults_{0}.csv'.format(modelName), index=False)\n",
    "wrongPredictions.to_csv('wrongPredictions_{0}.csv'.format(modelName), index=False)\n",
    "confusionMatrix.to_csv('confusionMatrix_{0}.csv'.format(modelName), index=False)\n",
    "predictions.to_csv('predictions_{0}.csv'.format(modelName), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
